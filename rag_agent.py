# -*- coding: utf-8 -*-
"""Rag Agent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h5prQENRHXzpwnJFpRI-6TkyQwdWgFAa
"""

!pip install -U langchain langchain-community chromadb sentence-transformers transformers torch --quiet

from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.llms import HuggingFacePipeline
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline

loader = TextLoader("/content/aitext.txt")
documents = loader.load()
splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=50)
docs = splitter.split_documents(documents)

embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
vectorstore = Chroma.from_documents(docs, embeddings)

model_name = "google/flan-t5-base"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

pipe = pipeline("text2text-generation", model=model, tokenizer=tokenizer, max_length=512)
llm = HuggingFacePipeline(pipeline=pipe)

custom_prompt = PromptTemplate.from_template(
    """Use the following context to answer the question.

    Context: {context}

    Question: {question}
    Answer:"""
)

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever(),
    chain_type_kwargs={"prompt": custom_prompt}
)

while True:
    query = input("Ask a question (type 'exit' to quit): ")
    if query.lower() == "exit":
        break
    answer = qa_chain.run(query)
    print("Answer:", answer)